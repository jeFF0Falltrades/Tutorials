{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "guilty-jordan",
   "metadata": {},
   "source": [
    "# Machine Learning Tutorial: Simple Bank/Credit Card Transaction Classification Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unique-electron",
   "metadata": {},
   "source": [
    "This is my first toy project after spending a few COVID months getting a baseline understanding of machine learning using Python, scikit-learn, and pandas.\n",
    "\n",
    "It is a very simple credit card and banking transaction classification tool.\n",
    "\n",
    "I'm sharing it here:\n",
    "1. To encourage others to dive in to machine learning, because libraries like scikit-learn, pandas, and others make ML more accessible (and fun) than ever before, especially to scrubs like me, and\n",
    "1. To gather feedback from SMEs and continously improve my approach, sharing those learnings with others\n",
    "\n",
    "### TL;DR\n",
    "My wife and I keep monthly budget sheets which include transactions to/from our bank account and various credit cards, and we use these to track spending across several categories so that we can see where our funds are going each month, and where we should cut/continue/invest.\n",
    "\n",
    "As you can imagine, it is tedious to label each individual transaction, so I gathered ~a year's worth of labeled data and used this notebook to train an ML model which could auto-classify transactions for me, regardless of source account (bank or credit cards).\n",
    "\n",
    "Now I can compute, analyze, and weep over our budget sheets in a matter of seconds every month (Great Success).\n",
    "\n",
    "### Details\n",
    "We use monthly budget sheets to track our finances by category of spending (e.g. groceries, gas, mortgage, dining out, etc.). \n",
    "\n",
    "Previously, we did this by sitting down at the start of the month and manually labeling transactions from the last month of spending. \n",
    "\n",
    "This was tedious, and while some credit card/banking services offer this kind of \"tagging\" already, they all use different categories, and not always the same ones that would be most valuable to us.\n",
    "\n",
    "We wanted something more consistent, so we went through the tedium for a while, collecting all of the labeled data for about 13 months.\n",
    "\n",
    "The below Jupyter notebook - split into respective \"Pipelines\" of work - will walk through the process of:\n",
    "\n",
    "1. Training a classifier from the year's worth of manually-labeled data\n",
    "1. Training a classifer from a continously-updated \"master\" set of data for future relearning\n",
    "1. Optimizing classifier parameters\n",
    "1. Classifying new, unlabeled data using the classifier\n",
    "1. Saving newly-labeled data to the master set, where it can be manually tweaked if inaccurate so that the classifier can be retrained\n",
    "\n",
    "The implementation details will be discussed throughout the notebook.\n",
    "\n",
    "### Important Notes and Sharing Feedback\n",
    "* You will see certain data marked as \"REDACTED\" throughout the notebook - this is just data which contains location or PII about me or my personal accounts that I chose to omit from the project; It does not impact operations, but there are certain places where - if you choose to play with this notebook - you will have to tailor the notebook to your specific bank/credit card account data. I will highlight these.\n",
    "* I'm still an ML noob, so please share constructive feedback with me @jeFF0Falltrades on Twitter, or feel free to leave an Issue/PR on this GitHub.\n",
    "\n",
    "Thanks!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supreme-pennsylvania",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "likely-wallpaper",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adverse-stations",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from glob import glob\n",
    "from joblib import dump, load\n",
    "from numpy import logspace\n",
    "from pandas import CategoricalDtype, concat, DataFrame, get_dummies, read_csv\n",
    "from pprint import pprint\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from re import compile\n",
    "\n",
    "\n",
    "# Used for auto-formatting in Jupyter Lab\n",
    "# Comment the line below to use lab_black in a Jupyter Notebook outside of Jupyter Lab\n",
    "%load_ext lab_black\n",
    "\n",
    "# Uncomment the line below for use in a Jupyter Notebook outside of Jupyter Lab\n",
    "# %load_ext nb_black"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leading-elimination",
   "metadata": {},
   "source": [
    "## Persistent Constants and Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "better-valentine",
   "metadata": {},
   "source": [
    "Below are constants and functions which will be referenced throughout the rest of the notebook.\n",
    "\n",
    "**Note** that you will have to change these values to match your account information and file structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "mathematical-complex",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to labeled and unlabeled CSVs\n",
    "LABELED_CSV_PATH = \"E:\\\\Budget\\\\labeled_data\\\\*.csv\"\n",
    "UNLABELED_CSV_PATH = \"E:\\\\Budget\\\\unlabeled_data\\\\*.csv\"\n",
    "\n",
    "# Path to the master data set CSV\n",
    "MASTER_CSV_PATH = \"master_data.csv\"\n",
    "\n",
    "# Path to the saved optimal classifier\n",
    "MODEL_FILE_NAME = \"selected_model.joblib\"\n",
    "\n",
    "# Enums used to track the different type of accounts where transactions will be sourced from\n",
    "# You can add/modify these as necessary\n",
    "class ACCOUNTS(Enum):\n",
    "    BANK = \"REDACTED_BANK_ACCT\"\n",
    "    CC_1 = \"REDACTED_CC_ACCT\"\n",
    "    CC_2 = \"REDACTED_CC_ACCT_2\"\n",
    "\n",
    "\n",
    "# Increments of charges to include as encoded features\n",
    "# Feel free to change the increments; These worked well for me\n",
    "CHARGE_INCREMENTS = [20, 50, 100, 500, 1000]\n",
    "CHARGE_UNDER = \"charge_under_{}\"\n",
    "CHARGE_OVER = \"charge_over_{}\"\n",
    "CHARGE_COLS = []\n",
    "for incr in CHARGE_INCREMENTS:\n",
    "    CHARGE_COLS.append(CHARGE_UNDER.format(incr))\n",
    "CHARGE_COLS.append(CHARGE_OVER.format(CHARGE_INCREMENTS[-1]))\n",
    "\n",
    "# All expected feature columns;\n",
    "# We will use this to preserve ordering as our saved model will require the\n",
    "# feature columns to be in the same order between runs\n",
    "FEAT_COLS = [\"description\"] + CHARGE_COLS + [\"is_credit\"]\n",
    "\n",
    "# Characters to ignore (exclude) from transaction descriptions and charge values, respectively\n",
    "IGNORED_TXN_CHARS = compile(r\"[^a-z0-9 ]\")\n",
    "IGNORED_CHARGE_CHARS = compile(r\"[-() ]\")\n",
    "\n",
    "# Terms to ignore from all transactins (e.g. payments from one account\n",
    "# to another, which aren't really \"transactions\" to be tracked)\n",
    "IGNORED_TXN_TERMS = [\n",
    "    \"REDACTED_CC_ACCT EPAYMENT\",\n",
    "    \"REDACTED_CC_ACCT_2 E-PAYMENT\",\n",
    "    \"MOBILE PAYMENT - THANK YOU\",\n",
    "    \"INTERNET PAYMENT - THANK YOU\",\n",
    "    \"TRANSFER\",\n",
    "]\n",
    "IGNORED_TXN_PATT = compile(\"|\".join(IGNORED_TXN_TERMS))\n",
    "\n",
    "# Delimiter to use with input CSVs and output CSVs;\n",
    "# I tend to use ';' b/c some transaction descriptions contain a comma\n",
    "# and charges will too, depending on currency format\n",
    "CSV_DELIM = \";\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supreme-thumbnail",
   "metadata": {},
   "source": [
    "**Note** that you may have to tweak these functions to successfully parse your own institutions' CSV formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "demographic-diameter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks if the given charge is a credit or debit from the account\n",
    "def check_if_credit(account, charge):\n",
    "    # Cast to string as a safety for the substitution op below\n",
    "    charge = str(charge)\n",
    "    # My credit card accounts use the format \"-$12.00\" for credits\n",
    "    if account.lower() != ACCOUNTS.BANK.value and \"-\" in charge:\n",
    "        return 1\n",
    "    # My bank account uses the format \"($12.00)\" for debits and \"$12.00\" for credits\n",
    "    if account.lower() == ACCOUNTS.BANK.value and \"(\" not in charge:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "# Performs label encoding for charges based on the increments provided as constants above\n",
    "def encode_charges(charge):\n",
    "    # Cast to string as a safety for the substitution op below\n",
    "    charge = str(charge)\n",
    "    # Remove ignored chars and replace any commas with periods to cast to float\n",
    "    charge = float(IGNORED_CHARGE_CHARS.sub(\"\", charge).replace(\",\", \".\"))\n",
    "    for incr in CHARGE_INCREMENTS:\n",
    "        if charge <= incr:\n",
    "            return CHARGE_UNDER.format(incr)  # e.g. \"charge_under_20\"\n",
    "    return CHARGE_OVER.format(CHARGE_INCREMENTS[-1])  # e.g. \"charge_over_1000\"\n",
    "\n",
    "\n",
    "# Preprocesses description texts by removing ignored characters\n",
    "def clean_text(text):\n",
    "    return IGNORED_TXN_CHARS.sub(\"\", text.lower())\n",
    "\n",
    "\n",
    "# Combines data from each transaction CSV into a consolidated DataFrame\n",
    "# You may have to modify this to account for differences among your accounts\n",
    "def dataframe_from_csvs(path, delim=\",\", labeled=False):\n",
    "    frames_to_combine = []\n",
    "    for csv_file in glob(path):\n",
    "        df = read_csv(csv_file, delimiter=delim)\n",
    "        # If the data is not labeled, we need to describe how to parse the CSV\n",
    "        if not labeled:\n",
    "            # My credit card accounts both already have these present\n",
    "            if \"Description\" in df and \"Amount\" in df:\n",
    "                # The presence of the \"Card Member\" column differentiates the 2 CC accounts\n",
    "                if \"Card Member\" in df:\n",
    "                    df[\"Account\"] = ACCOUNTS.CC_1.value\n",
    "                else:\n",
    "                    df[\"Account\"] = ACCOUNTS.CC_2.value\n",
    "                frames_to_combine.append(df[[\"Account\", \"Description\", \"Amount\"]])\n",
    "            # My bank account, on the other hand, has a unique \"*Beginning Balance*\" column\n",
    "            elif \"*Beginning Balance*\" in df:\n",
    "                new_df = DataFrame()\n",
    "                new_df[\"Description\"] = df[\"*Beginning Balance*\"][:-1]\n",
    "                new_df[\"Amount\"] = df.iloc[:, 3][:-1]\n",
    "                new_df[\"Account\"] = ACCOUNTS.BANK.value\n",
    "                frames_to_combine.append(new_df)\n",
    "        # If the data is already labeled, we only need to combine it to the other frames\n",
    "        else:\n",
    "            frames_to_combine.append(df)\n",
    "    df = concat(frames_to_combine, axis=0, ignore_index=True)\n",
    "    # Remove ignored transactions using regex from constants above\n",
    "    df = df[~df[\"Description\"].str.contains(IGNORED_TXN_PATT)]\n",
    "    # Reset the index (row identifier) as if we preserve the random indices of\n",
    "    # the separate data sets, it can cause problems during preprocessing\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Runs preprocessing to derive input features (X) from transaction data\n",
    "def derive_X(df, acct_col_label, desc_col_label, charge_col_label):\n",
    "    X_data = DataFrame()\n",
    "    # Normalize description texts\n",
    "    X_data[\"description\"] = raw_df.apply(\n",
    "        lambda row: clean_text(row[desc_col_label]), axis=1\n",
    "    )\n",
    "    # Label-encode charges using increments above\n",
    "    X_data[\"charges\"] = raw_df.apply(\n",
    "        lambda row: encode_charges(row[charge_col_label]), axis=1\n",
    "    )\n",
    "    # If our data is missing any of the charge categories, this op will add\n",
    "    # the missing column and set it to 0 for all rows, ensuring consistency\n",
    "    X_data[\"charges\"] = X_data[\"charges\"].astype(\n",
    "        CategoricalDtype(categories=CHARGE_COLS)\n",
    "    )\n",
    "    # One-hot encode the label-encoded charges to split them out into\n",
    "    # 1 column / 1 charge increment.\n",
    "    # get_dummies() will do this for us easily:\n",
    "    # https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html\n",
    "    one_hot_charges = get_dummies(\n",
    "        X_data[[\"charges\"]], prefix=\"\", prefix_sep=\"\", columns=[\"charges\"]\n",
    "    )\n",
    "    # Drop the label-encoded charges, and join the one-hot encoded charges\n",
    "    X_data = X_data.drop(\"charges\", axis=1).join(one_hot_charges)\n",
    "    # Capture if transaction is a credit or debit\n",
    "    X_data[\"is_credit\"] = raw_df.apply(\n",
    "        lambda row: check_if_credit(row[acct_col_label], row[charge_col_label]), axis=1\n",
    "    )\n",
    "    # This line ensures ordering is preserved as our saved model will expect\n",
    "    # the same column order each run\n",
    "    X_data = X_data[FEAT_COLS]\n",
    "    return X_data\n",
    "\n",
    "\n",
    "# Add data from a frame to the master data set for later retraining\n",
    "def add_data_to_master_set(df):\n",
    "    # Only write headers if master CSV already exists\n",
    "    write_header = not glob(MASTER_CSV_PATH)\n",
    "    df.to_csv(\n",
    "        MASTER_CSV_PATH, mode=\"a\", index=False, sep=CSV_DELIM, header=write_header\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chemical-government",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dated-breath",
   "metadata": {},
   "source": [
    "## Pipeline 1: Retrieve Training Data From Manually-Labeled Transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threaded-biodiversity",
   "metadata": {},
   "source": [
    "This pipeline is used to train our classifier using *manually-labeled* data, e.g. the year's worth of manually labeled transaction CSVs I compiled.\n",
    "\n",
    "Those CSVs were in the below format with a `';'` delimiter, but you can adjust the code to fit whichever format your account CSVs use:\n",
    "\n",
    "|   Date  | Account |                  Description                 | Charge | Direction |    Category   |\n",
    "|:-------:|:-------:|:--------------------------------------------:|:------:|:---------:|:-------------:|\n",
    "| January |   REDACTED_CC_ACCT  |                  McDonald's                  |  5,06  |   Debit   |   Dining Out  |\n",
    "| January |   REDACTED_CC_ACCT  |                   Michaels                   |  11,26 |   Debit   |   Household   |\n",
    "| January |   REDACTED_CC_ACCT  |                   T.J. Maxx                  |  10,69 |   Debit   |   Household   |\n",
    "| January |   REDACTED_CC_ACCT  |                    USPS PO                   |  23,93 |   Debit   |    Shipping   |\n",
    "| January |   REDACTED_CC_ACCT  |                  Which Wich                  |  21,01 |   Debit   |   Dining Out  |\n",
    "| January |   REDACTED_CC_ACCT  | APPLE.COM/BILL      INTERNET   CHARGE     CA |  0,99  |   Debit   | Entertainment |\n",
    "|  March  |   REDACTED_CC_ACCT  | CHEWY.COM             (800)672-4399       FL |  31,96 |   Debit   |      Pets     |\n",
    "|  March  |  REDACTED_BANK_ACCT |           MOBILE DEPOSIT AUTO-POST           | 100.00 |   Credit  |     Gifts     |\n",
    "|   ...   |   ...   |                      ...                     |   ...  |    ...    |      ...      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "secondary-delicious",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1955, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Account</th>\n",
       "      <th>Description</th>\n",
       "      <th>Charge</th>\n",
       "      <th>Direction</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>August</td>\n",
       "      <td>REDACTED_CC_ACCT_2</td>\n",
       "      <td>REDACTED_WATER_UTILITY</td>\n",
       "      <td>75.43</td>\n",
       "      <td>Debit</td>\n",
       "      <td>Utilities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>August</td>\n",
       "      <td>REDACTED_CC_ACCT_2</td>\n",
       "      <td>REDACTED_WATER_UTILITY</td>\n",
       "      <td>2.99</td>\n",
       "      <td>Debit</td>\n",
       "      <td>Utilities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>August</td>\n",
       "      <td>REDACTED_CC_ACCT_2</td>\n",
       "      <td>RAISING CANES REDACTED_STATE</td>\n",
       "      <td>15.91</td>\n",
       "      <td>Debit</td>\n",
       "      <td>Dining Out</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>August</td>\n",
       "      <td>REDACTED_CC_ACCT_2</td>\n",
       "      <td>BURGER KING  REDACTED_STATE</td>\n",
       "      <td>16.57</td>\n",
       "      <td>Debit</td>\n",
       "      <td>Dining Out</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>August</td>\n",
       "      <td>REDACTED_CC_ACCT_2</td>\n",
       "      <td>MEIJER REDACTED_STATE</td>\n",
       "      <td>84.93</td>\n",
       "      <td>Debit</td>\n",
       "      <td>Groceries</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Date   Account                             Description Charge Direction  \\\n",
       "0  August  REDACTED_CC_ACCT_2  REDACTED_WATER_UTILITY  75.43     Debit   \n",
       "1  August  REDACTED_CC_ACCT_2   REDACTED_WATER_UTILITY  2.99     Debit   \n",
       "2  August  REDACTED_CC_ACCT_2       RAISING CANES REDACTED_STATE  15.91     Debit   \n",
       "3  August  REDACTED_CC_ACCT_2         BURGER KING  REDACTED_STATE  16.57     Debit   \n",
       "4  August  REDACTED_CC_ACCT_2              MEIJER REDACTED_STATE  84.93     Debit   \n",
       "\n",
       "     Category  \n",
       "0   Utilities  \n",
       "1   Utilities  \n",
       "2  Dining Out  \n",
       "3  Dining Out  \n",
       "4   Groceries  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df = dataframe_from_csvs(LABELED_CSV_PATH, delim=CSV_DELIM, labeled=True)\n",
    "# FYI: df.shape shows (rows, columns) counts for the data frame\n",
    "print(raw_df.shape)\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "double-somalia",
   "metadata": {},
   "source": [
    "Now let's take the raw labeled data, and run it through our `derive_X()` function to encode the features we want to train our classifier on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "artificial-reconstruction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1955, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>charge_under_20</th>\n",
       "      <th>charge_under_50</th>\n",
       "      <th>charge_under_100</th>\n",
       "      <th>charge_under_500</th>\n",
       "      <th>charge_under_1000</th>\n",
       "      <th>charge_over_1000</th>\n",
       "      <th>is_credit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>REDACTED_WATER_UTILITY</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>REDACTED_WATER_UTILITY</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>raising canes REDACTED_STATE</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>burger king REDACTED_STATE</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>meijer REDACTED_STATE</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             description  charge_under_20  charge_under_50  \\\n",
       "0  REDACTED_WATER_UTILITY                0                0   \n",
       "1    REDACTED_WATER_UTILITY                1                0   \n",
       "2      raising canes REDACTED_STATE                1                0   \n",
       "3         burger king REDACTED_STATE                1                0   \n",
       "4              meijer REDACTED_STATE                0                0   \n",
       "\n",
       "   charge_under_100  charge_under_500  charge_under_1000  charge_over_1000  \\\n",
       "0                 1                 0                  0                 0   \n",
       "1                 0                 0                  0                 0   \n",
       "2                 0                 0                  0                 0   \n",
       "3                 0                 0                  0                 0   \n",
       "4                 1                 0                  0                 0   \n",
       "\n",
       "   is_credit  \n",
       "0          0  \n",
       "1          0  \n",
       "2          0  \n",
       "3          0  \n",
       "4          0  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data = derive_X(raw_df, \"Account\", \"Description\", \"Charge\")\n",
    "print(X_data.shape)\n",
    "X_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "progressive-replacement",
   "metadata": {},
   "source": [
    "Next, for training our classifier, we need to tell it the actual labels we (humans) used for the training transactions, so it can start learning how the features (X) relate to the target labels (y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "indonesian-outreach",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1955,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0     utilities\n",
       "1     utilities\n",
       "2    dining out\n",
       "3    dining out\n",
       "4     groceries\n",
       "Name: Category, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data = raw_df[\"Category\"].str.lower()\n",
    "print(y_data.shape)\n",
    "y_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifteen-integration",
   "metadata": {},
   "source": [
    "Before moving on to training the classifier, now that we have already transformed our raw, manually-labeled data into the format our model will expect, we can save this transformed data to our master data set.\n",
    "\n",
    "This way, we can go back and tweak the data later, then retrain the model using **Pipeline 2** so we can skip some of the preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "therapeutic-imaging",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1955, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>charge_under_20</th>\n",
       "      <th>charge_under_50</th>\n",
       "      <th>charge_under_100</th>\n",
       "      <th>charge_under_500</th>\n",
       "      <th>charge_under_1000</th>\n",
       "      <th>charge_over_1000</th>\n",
       "      <th>is_credit</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>REDACTED_WATER_UTILITY</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>utilities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>REDACTED_WATER_UTILITY</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>utilities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>raising canes REDACTED_STATE</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dining out</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>burger king REDACTED_STATE</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dining out</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>meijer REDACTED_STATE</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>groceries</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             description  charge_under_20  charge_under_50  \\\n",
       "0  REDACTED_WATER_UTILITY                0                0   \n",
       "1    REDACTED_WATER_UTILITY                1                0   \n",
       "2      raising canes REDACTED_STATE                1                0   \n",
       "3         burger king REDACTED_STATE                1                0   \n",
       "4              meijer REDACTED_STATE                0                0   \n",
       "\n",
       "   charge_under_100  charge_under_500  charge_under_1000  charge_over_1000  \\\n",
       "0                 1                 0                  0                 0   \n",
       "1                 0                 0                  0                 0   \n",
       "2                 0                 0                  0                 0   \n",
       "3                 0                 0                  0                 0   \n",
       "4                 1                 0                  0                 0   \n",
       "\n",
       "   is_credit    category  \n",
       "0          0   utilities  \n",
       "1          0   utilities  \n",
       "2          0  dining out  \n",
       "3          0  dining out  \n",
       "4          0   groceries  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write the preprocessed labeled data to the master data set for inclusion in future retraining\n",
    "# This includes our preprocessed features (X) and target values (y)\n",
    "combined_df = X_data.join(y_data)\n",
    "combined_df.columns = combined_df.columns.str.lower()\n",
    "print(combined_df.shape)\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dying-insured",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_data_to_master_set(combined_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outstanding-princess",
   "metadata": {},
   "source": [
    "### Skip to Pipeline 3, unless training with Master Data Set as well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emotional-trinity",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "furnished-arabic",
   "metadata": {},
   "source": [
    "## Pipeline 2: Retrieve Training Data From Master Data Set (Manually + Automatically Labeled Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divided-thing",
   "metadata": {},
   "source": [
    "As opposed to **Pipeline 1**, which relies on a different format of manually-labeled data, once we start building a \"master\" data set of our desired features (X) and assigned categories (y), we can train or retrain our classifier using this data without any preprocessing needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "useful-greenhouse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2315, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>charge_under_20</th>\n",
       "      <th>charge_under_50</th>\n",
       "      <th>charge_under_100</th>\n",
       "      <th>charge_under_500</th>\n",
       "      <th>charge_under_1000</th>\n",
       "      <th>charge_over_1000</th>\n",
       "      <th>is_credit</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>REDACTED_WATER_UTILITY</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>utilities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>REDACTED_WATER_UTILITY</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>utilities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>raising canes REDACTED_STATE</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dining out</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>burger king REDACTED_STATE</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dining out</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>meijer REDACTED_STATE</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>groceries</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             description  charge_under_20  charge_under_50  \\\n",
       "0  REDACTED_WATER_UTILITY                0                0   \n",
       "1    REDACTED_WATER_UTILITY                1                0   \n",
       "2      raising canes REDACTED_STATE                1                0   \n",
       "3         burger king REDACTED_STATE                1                0   \n",
       "4              meijer REDACTED_STATE                0                0   \n",
       "\n",
       "   charge_under_100  charge_under_500  charge_under_1000  charge_over_1000  \\\n",
       "0                 1                 0                  0                 0   \n",
       "1                 0                 0                  0                 0   \n",
       "2                 0                 0                  0                 0   \n",
       "3                 0                 0                  0                 0   \n",
       "4                 1                 0                  0                 0   \n",
       "\n",
       "   is_credit    category  \n",
       "0          0   utilities  \n",
       "1          0   utilities  \n",
       "2          0  dining out  \n",
       "3          0  dining out  \n",
       "4          0   groceries  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load labeled master data set for training/retraining\n",
    "raw_df = read_csv(MASTER_CSV_PATH, delimiter=CSV_DELIM)\n",
    "print(raw_df.shape)\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "stone-preparation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2315, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>charge_under_20</th>\n",
       "      <th>charge_under_50</th>\n",
       "      <th>charge_under_100</th>\n",
       "      <th>charge_under_500</th>\n",
       "      <th>charge_under_1000</th>\n",
       "      <th>charge_over_1000</th>\n",
       "      <th>is_credit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>REDACTED_WATER_UTILITY</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>REDACTED_WATER_UTILITY</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>raising canes REDACTED_STATE</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>burger king REDACTED_STATE</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>meijer REDACTED_STATE</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             description  charge_under_20  charge_under_50  \\\n",
       "0  REDACTED_WATER_UTILITY                0                0   \n",
       "1    REDACTED_WATER_UTILITY                1                0   \n",
       "2      raising canes REDACTED_STATE                1                0   \n",
       "3         burger king REDACTED_STATE                1                0   \n",
       "4              meijer REDACTED_STATE                0                0   \n",
       "\n",
       "   charge_under_100  charge_under_500  charge_under_1000  charge_over_1000  \\\n",
       "0                 1                 0                  0                 0   \n",
       "1                 0                 0                  0                 0   \n",
       "2                 0                 0                  0                 0   \n",
       "3                 0                 0                  0                 0   \n",
       "4                 1                 0                  0                 0   \n",
       "\n",
       "   is_credit  \n",
       "0          0  \n",
       "1          0  \n",
       "2          0  \n",
       "3          0  \n",
       "4          0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With the already-transformed data, we just have to drop the \"category\" column to get X!\n",
    "X_data = raw_df.loc[:, raw_df.columns != \"category\"]\n",
    "print(X_data.shape)\n",
    "X_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "moving-dinner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2315,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0     utilities\n",
       "1     utilities\n",
       "2    dining out\n",
       "3    dining out\n",
       "4     groceries\n",
       "Name: category, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Boom, instant labels as well\n",
    "y_data = raw_df[\"category\"].str.lower()\n",
    "print(y_data.shape)\n",
    "y_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "criminal-colombia",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assumed-aggregate",
   "metadata": {},
   "source": [
    "## Pipeline 3 (Follows PL1 or PL2): Train Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medical-convergence",
   "metadata": {},
   "source": [
    "Now, we get to the most important and extensive pipeline of our notebook: Training our classifier.\n",
    "\n",
    "Using the data from **Pipeline 1** or **Pipeline 2** above (or both), we split the data into a [\"training\" and \"testing\" set](https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets).\n",
    "\n",
    "scikit-learn contains a great function - [`train_test_split()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) - for just this purpose.\n",
    "\n",
    "Notice that we're not using a validation set here, to keep things simple, but as a personal exercise, you could also build a validation set to run tests prior to selecting a final, fully-tuned model (for an example of this, see [here](https://datascience.stackexchange.com/questions/15135/train-test-validation-set-splitting-in-sklearn)).\n",
    "\n",
    "<br>\n",
    "\n",
    "We split using an 80/20 train/test ratio.\n",
    "\n",
    "Why split the data like this at all?\n",
    "\n",
    "So our classifier doesn't adapt so well to the training data set that it doesn't generalize to future unlabeled data sets:\n",
    "\n",
    "The training set will be used to \"train\" (obviously) our classifier, which is why it typically is made up of the lion's share of the total available data, and we will analyze it to tune our model.\n",
    "\n",
    "The test set, on the other hand, is meant to serve as a separate, untouched and unseen (by us) data set to test our final tweaked and trained model on in order to ensure the model didn't overfit (\"memorize\") the training data, and will generalize enough to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "designed-zimbabwe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_data, y_data, train_size=0.8, test_size=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beneficial-freight",
   "metadata": {},
   "source": [
    "To classify our transactions, we are going to use 3 high-level features:\n",
    "\n",
    "1. The description of the transaction (provided by the bank/credit card CSV)\n",
    "1. The amount of the transaction (encoded using our intervals above)\n",
    "1. Whether the transaction is considered a debit or credit\n",
    "\n",
    "Let's look at these in more detail:\n",
    "\n",
    "### Description of the Transaction\n",
    "\n",
    "We will preprocess the description text to remove superfluous characters, then remove unwanted transactions (see `IGNORED_TXN_TERMS` above), and then use [term frequencyâ€“inverse document frequency or TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) to encode the words in the description into a [sparse matrix](https://machinelearningmastery.com/sparse-matrices-for-machine-learning/#:~:text=A%20sparse%20matrix%20is%20a,of%20its%20coefficients%20are%20zero) of features.\n",
    "\n",
    "Once again, scikit-learn has an awesome prebuilt function for this - [`TfidfVectorizer()`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html).\n",
    "\n",
    "Put simply, TF-IDF is used to weigh the importance of words across several documents or - in our case - transaction descriptions.\n",
    "\n",
    "It will analyze each \"word\" (which may not equate to an actual English word, so \"term\" or \"token\" can also be used) of the preprocessed transaction description and rank words that appear often in a particular description as more important in classifying that particular category of transaction, **unless** the term(s) also appear across other descriptions, in which case their importance is weighted less (terms like `\"USA\"` or `\"Charge\"`).\n",
    "\n",
    "Instead of actual English \"words\", however, `TfidfVectorizer()` can be modified to use a range of n-grams, which essentially include any string of non-space characters (though this can be modified by tweaking various parameters of the function to define what a \"token\" should be considered, which \"stop words\" (common terms) should be filtered, and more).\n",
    "\n",
    "For example, given argument `ngram_range=(1,2)` (unigrams and bigrams) a description text like `\"marketplace credit charge us\"` will be vectorized to: `['marketplace', 'credit', 'charge' ,'us', 'marketplace credit', 'credit charge', 'charge us']`.\n",
    "\n",
    "This vectorization will produce a sparse matrix which will be passed through to our classifier, so that it can learn which tokens/words in the transaction descriptions are associated with a particular category.\n",
    "\n",
    "For example, the tokens `\"Burger\"` and `\"Burger King\"` are very likely going to be strongly associated with the `\"Dining Out\"` category :-).\n",
    "\n",
    "We will also make use of scikit-learn's built-in [`ColumnTransformer()`](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html) function, which makes it easy for us to run certain transformers (like `TfidfVectorizer`) on certain columns, and run different transformers - or just pass through \"as-is\", which we will do here - on remaining columns.\n",
    "\n",
    "### Amount of the transaction\n",
    "Why go through the trouble of tracking the amount of the transaction?\n",
    "\n",
    "I chose to do this in order to increase the accuracy of the model, specifically finding it useful for the case when faced with transactions from the same shop/org, but in different increments.\n",
    "\n",
    "For example, sometimes, when I get fuel at a gas station when traveling, I'll also stop in to grab a drink or snack.\n",
    "\n",
    "Both of these transaction descriptions will probably include the name of the gas station, but one will be smaller than the other (unless I'm **really** loading up on snacks for a long trip).\n",
    "\n",
    "Our model might try to classify both of these as `\"Gas\"` based on the presence of the gas station name in the charge. \n",
    "\n",
    "But by including information about the charge amount, smaller charges will be recognized as `\"Groceries\"` (really, \"snacks\") and larger charges will be classified as `\"Gas\"`.\n",
    "\n",
    "### Transaction is Credit or Debit\n",
    "Once again, without including information about whether a transaction is a credit or debit, the model might learn to categorize both something I bought from Home Depot as `\"Household\"` as well as a refund on the same item if I later return it.\n",
    "\n",
    "Obviously, I want these tracked as two different transactions: The debit as a `\"Household\"` transaction, and the refund as a `\"Refund\"` transaction.\n",
    "\n",
    "Adding in a feature to track which charges are debits or credits helps train our classifier to differentiate positive/negative charges appropriately.\n",
    "\n",
    "<br>\n",
    "\n",
    "That was a lot of background info for just a couple of lines of code, but hopefully it helps understand the approach!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "technological-domain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build TF-IDF pipeline for description texts, passing other columns \"as-is\"\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "ct = ColumnTransformer([(\"tfidf\", tfidf_vec, \"description\")], remainder=\"passthrough\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "therapeutic-meditation",
   "metadata": {},
   "source": [
    "### Training and Tuning\n",
    "For classification, I'm going to use Linear Support Vector Classification, built into scikit-learn as [`LinearSVC()`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html).\n",
    "\n",
    "<br>\n",
    "\n",
    "Some resources on Support Vector Machines and Classification:\n",
    "\n",
    "By definition, [Support Vector Machines](https://medium.com/axum-labs/logistic-regression-vs-support-vector-machines-svm-c335610a3d16) (SVMs) are supervised learning models which try to find a maximum *margin* between the line (in 2D space) or hyperplane (in 3D+ space) separating classes of items. By maximizing that margin, we can find the optimal separation of those classes of items (think of the classes as our \"categories\" of charges).\n",
    "\n",
    "The best ELI5 I've seen for SVMs is /u/copperking's [here](https://www.reddit.com/r/MachineLearning/comments/15zrpp/please_explain_support_vector_machines_svm_like_i/), complete with helpful images.\n",
    "\n",
    "There are also two great blogs from [Georgios Drakos](https://gdcoder.com/support-vector-machine-vs-logistic-regression/) and [Lilly Chen](https://towardsdatascience.com/support-vector-machine-simply-explained-fee28eba5496) that offer a primer on SVMs and how they compare to logistic regression algorithms.\n",
    "\n",
    "<br>\n",
    "\n",
    "Prior to settling on using `LinearSVC`, I ran tests with the following classifier algorithms built into scikit-learn:\n",
    "\n",
    "* Regular SVC - `SVC()`\n",
    "* Logistic Regression  - `LogisticRegression()`\n",
    "* Random Forest - `RandomForestClassifier()`\n",
    "\n",
    "Each of these can be wrapped in [`OneVsRestClassifier()` or `OneVsOneClassifier()`](https://machinelearningmastery.com/one-vs-rest-and-one-vs-one-for-multi-class-classification/) from scikit-learn for multi-class classification problems like the one we are solving here, though we would have to modify our y target values to be one-hot encoded.\n",
    "\n",
    "See the cell at the bottom of this notebook for just such an example, with some tests of these classifiers.\n",
    "\n",
    "<br>\n",
    "\n",
    "I ultimately chose `LinearSVC()` because it trains quickly, scales well, and has a decent F1 score compared to the other classifiers I tested.\n",
    "\n",
    "I chose to evaluate scoring via the `f1_macro` metric due to the imbalance of classes/categories in this data set.\n",
    "\n",
    "Please see [here](https://datascience.stackexchange.com/questions/40900/whats-the-difference-between-sklearn-f1-score-micro-and-weighted-for-a-mult) for a great explanation of various F1 metrics in scikit-learn, and [here](https://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/) for an explanation of F1 Scoring in general, as well as other important scoring metrics like Accuracy, Precision, and Recall.\n",
    "\n",
    "<br>\n",
    "\n",
    "For my data set, I received some warnings along the lines of the below.\n",
    "\n",
    "Both of these are due to some categories only being used once in the training dataset (for example `\"home down payment\"` is not really an oft-occurring category of transaction), which can impact how well the classifier can learn the features that define that category:\n",
    "\n",
    "* `UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5`\n",
    "* `UserWarning: Label not (N) is present in all training examples.`\n",
    "\n",
    "This is okay, however - the classifier will still attempt to make a best prediction, and as time goes on and we gather more data, relabel misclassified data, and retrain our model, it will become more accurate.\n",
    "\n",
    "<br/>\n",
    "\n",
    "We will \"tune\" the hyperparameters of our TF-IDF Vectorizer and Linear SVC classifier using [`GridSearchCV()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html), another awesome built-in of scikit-learn which we can provide a range of values to test as parameters to our pipeline, and which will evaluate these parameters and output the best-performing model based on our scoring metric (F1 Macro).\n",
    "\n",
    "The current F1 score falls between the .65-.75 range - not fantastic, but I don't consider it too bad because I was less structured in how I categorized transactions early on, and so there are many categories that only appear once or a couple of times.\n",
    "\n",
    "When running the model on 3 months of new, unlabeled data, I actually only had to relabel <2% of transactions which were classified incorrectly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "tired-celebrity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize classifier\n",
    "cf = LinearSVC()\n",
    "\n",
    "# Prepare parameters to optimize using Cross Validation\n",
    "param_grid = {\n",
    "    \"coltrans__tfidf__ngram_range\": [(1, 1), (1, 2), (1, 3), (1, 4)],\n",
    "    \"coltrans__tfidf__norm\": [\"l1\", \"l2\"],\n",
    "    \"clf__C\": logspace(-3, 3, 13),\n",
    "}\n",
    "\n",
    "# Build pipeline for TF-IDF + OVR/SVC, using accuracy as scoring metric\n",
    "pipe = Pipeline([(\"coltrans\", ct), (\"clf\", cf)])\n",
    "clf = GridSearchCV(pipe, param_grid=param_grid, scoring=\"f1_macro\", n_jobs=-1)\n",
    "\n",
    "# Find best performing parameters\n",
    "best_clf = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "powered-machinery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model F1 Score is 0.7163354685138132\n",
      "\n",
      "\n",
      "{'clf': LinearSVC(C=3.1622776601683795),\n",
      " 'clf__C': 3.1622776601683795,\n",
      " 'clf__class_weight': None,\n",
      " 'clf__dual': True,\n",
      " 'clf__fit_intercept': True,\n",
      " 'clf__intercept_scaling': 1,\n",
      " 'clf__loss': 'squared_hinge',\n",
      " 'clf__max_iter': 1000,\n",
      " 'clf__multi_class': 'ovr',\n",
      " 'clf__penalty': 'l2',\n",
      " 'clf__random_state': None,\n",
      " 'clf__tol': 0.0001,\n",
      " 'clf__verbose': 0,\n",
      " 'coltrans': ColumnTransformer(remainder='passthrough',\n",
      "                  transformers=[('tfidf', TfidfVectorizer(ngram_range=(1, 3)),\n",
      "                                 'description')]),\n",
      " 'coltrans__n_jobs': None,\n",
      " 'coltrans__remainder': 'passthrough',\n",
      " 'coltrans__sparse_threshold': 0.3,\n",
      " 'coltrans__tfidf': TfidfVectorizer(ngram_range=(1, 3)),\n",
      " 'coltrans__tfidf__analyzer': 'word',\n",
      " 'coltrans__tfidf__binary': False,\n",
      " 'coltrans__tfidf__decode_error': 'strict',\n",
      " 'coltrans__tfidf__dtype': <class 'numpy.float64'>,\n",
      " 'coltrans__tfidf__encoding': 'utf-8',\n",
      " 'coltrans__tfidf__input': 'content',\n",
      " 'coltrans__tfidf__lowercase': True,\n",
      " 'coltrans__tfidf__max_df': 1.0,\n",
      " 'coltrans__tfidf__max_features': None,\n",
      " 'coltrans__tfidf__min_df': 1,\n",
      " 'coltrans__tfidf__ngram_range': (1, 3),\n",
      " 'coltrans__tfidf__norm': 'l2',\n",
      " 'coltrans__tfidf__preprocessor': None,\n",
      " 'coltrans__tfidf__smooth_idf': True,\n",
      " 'coltrans__tfidf__stop_words': None,\n",
      " 'coltrans__tfidf__strip_accents': None,\n",
      " 'coltrans__tfidf__sublinear_tf': False,\n",
      " 'coltrans__tfidf__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      " 'coltrans__tfidf__tokenizer': None,\n",
      " 'coltrans__tfidf__use_idf': True,\n",
      " 'coltrans__tfidf__vocabulary': None,\n",
      " 'coltrans__transformer_weights': None,\n",
      " 'coltrans__transformers': [('tfidf',\n",
      "                             TfidfVectorizer(ngram_range=(1, 3)),\n",
      "                             'description')],\n",
      " 'coltrans__verbose': False,\n",
      " 'memory': None,\n",
      " 'steps': [('coltrans',\n",
      "            ColumnTransformer(remainder='passthrough',\n",
      "                  transformers=[('tfidf', TfidfVectorizer(ngram_range=(1, 3)),\n",
      "                                 'description')])),\n",
      "           ('clf', LinearSVC(C=3.1622776601683795))],\n",
      " 'verbose': False}\n"
     ]
    }
   ],
   "source": [
    "# Print best parameter/estimator data and model accuracy\n",
    "print(\"Model F1 Score is {}\\n\\n\".format(best_clf.score(X_test, y_test)))\n",
    "pprint(best_clf.best_estimator_.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "prescription-sapphire",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can save our optimized model to disk for later reloading\n",
    "dump(best_clf, open(MODEL_FILE_NAME, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equivalent-catch",
   "metadata": {},
   "source": [
    "To read and experiment with the test data predictions, run the following cell and skip to **Pipeline 5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capable-richmond",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offshore-radio",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understood-finland",
   "metadata": {},
   "source": [
    "## Pipeline 4: Classify New Unlabeled Data Using the Optimal Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geographic-mystery",
   "metadata": {},
   "source": [
    "Okay, now with all of that pre-work out of the way, we get to the easy and fun part: Classifying new, unlabeled data with our trained model!\n",
    "\n",
    "Instead of manual CSVs that I put together and labeled, this unlabeled data is derived from a set of CSVs directly downloaded from my banking/credit card sites and unmodified.\n",
    "\n",
    "Most every banking/card service I know of will allow you to download your monthly transactions as a CSV - usually this option is listed on your \"Statement\" or \"Search Transactions\" pages as a \"Download\" option, at which point it will ask you which format to download to.\n",
    "\n",
    "The function `dataframe_from_csvs()` was written specifically to parse my bank/credit card transaction CSVs, so you will need to tweak it to your services accordingly.\n",
    "\n",
    "The essential piece is to derive the account type, transaction description, and transaction amount. That's all our pipeline will need to classify the new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "sorted-venezuela",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(147, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Account</th>\n",
       "      <th>Description</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>REDACTED_CC_ACCT</td>\n",
       "      <td>AplPay MEIJER REDACTED_STATE</td>\n",
       "      <td>42.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>REDACTED_CC_ACCT</td>\n",
       "      <td>REDACTED_CC_OFFER</td>\n",
       "      <td>-43.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>REDACTED_CC_ACCT</td>\n",
       "      <td>AplPay TARGET REDACTED_STATE</td>\n",
       "      <td>9.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>REDACTED_CC_ACCT</td>\n",
       "      <td>NOODLES &amp; CO  REDACTED_STATE</td>\n",
       "      <td>31.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>REDACTED_CC_ACCT</td>\n",
       "      <td>AplPay IKEA REDACTED_STATE</td>\n",
       "      <td>42.58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Account                                 Description Amount\n",
       "0    REDACTED_CC_ACCT  AplPay MEIJER REDACTED_STATE  42.97\n",
       "1    REDACTED_CC_ACCT              REDACTED_CC_OFFER -43.12\n",
       "2    REDACTED_CC_ACCT  AplPay TARGET REDACTED_STATE   9.08\n",
       "3    REDACTED_CC_ACCT  NOODLES & CO  REDACTED_STATE  31.42\n",
       "4    REDACTED_CC_ACCT  AplPay IKEA REDACTED_STATE 42.58"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df = dataframe_from_csvs(UNLABELED_CSV_PATH)\n",
    "print(raw_df.shape)\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "monthly-output",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(147, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>charge_under_20</th>\n",
       "      <th>charge_under_50</th>\n",
       "      <th>charge_under_100</th>\n",
       "      <th>charge_under_500</th>\n",
       "      <th>charge_under_1000</th>\n",
       "      <th>charge_over_1000</th>\n",
       "      <th>is_credit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aplpay meijer REDACTED_STATE</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>REDACTED_CC_OFFER</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aplpay target REDACTED_STATE</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>noodles  co  REDACTED_STATE</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aplpay ikea REDACTED_STATE</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  description  charge_under_20  \\\n",
       "0   aplpay meijer REDACTED_STATE                0   \n",
       "1              REDACTED_CC_OFFER                0   \n",
       "2  aplpay target REDACTED_STATE                1   \n",
       "3   noodles  co  REDACTED_STATE                0   \n",
       "4  aplpay ikea REDACTED_STATE               0   \n",
       "\n",
       "   charge_under_50  charge_under_100  charge_under_500  charge_under_1000  \\\n",
       "0                1                 0                 0                  0   \n",
       "1                1                 0                 0                  0   \n",
       "2                0                 0                 0                  0   \n",
       "3                1                 0                 0                  0   \n",
       "4                1                 0                 0                  0   \n",
       "\n",
       "   charge_over_1000  is_credit  \n",
       "0                 0          0  \n",
       "1                 0          1  \n",
       "2                 0          0  \n",
       "3                 0          0  \n",
       "4                 0          0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = derive_X(raw_df, \"Account\", \"Description\", \"Amount\")\n",
    "print(X.shape)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strange-editor",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "human-vault",
   "metadata": {},
   "source": [
    "## Pipeline 5: View Classified Data and (Optionally) Add To Master Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "junior-optics",
   "metadata": {},
   "source": [
    "Whether coming from **Pipeline 4** or viewing our test data from **Pipeline 3**, we can now load our saved model from disk, call its `predict()` function on our unlabeled data, and then join these predictions to our input data.\n",
    "\n",
    "Then we can choose to save this data to disk into our master data set, as well as a more human-readable CSV for a quick read-through and/or relabeling of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "relevant-classics",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = load(MODEL_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cleared-amino",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predicted = clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "exempt-motivation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(147, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>charge_under_20</th>\n",
       "      <th>charge_under_50</th>\n",
       "      <th>charge_under_100</th>\n",
       "      <th>charge_under_500</th>\n",
       "      <th>charge_under_1000</th>\n",
       "      <th>charge_over_1000</th>\n",
       "      <th>is_credit</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aplpay meijer REDACTED_STATE</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>groceries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>REDACTED_CC_OFFER</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>REDACTED_CC_ACCT credit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aplpay target REDACTED_STATE</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>household</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>noodles  co  REDACTED_STATE</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dining out</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aplpay ikea REDACTED_STATE</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>household</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  description  charge_under_20  \\\n",
       "0   aplpay meijer REDACTED_STATE                0   \n",
       "1              REDACTED_CC_OFFER                0   \n",
       "2  aplpay target REDACTED_STATE                1   \n",
       "3   noodles  co  REDACTED_STATE                0   \n",
       "4  aplpay ikea REDACTED_STATE               0   \n",
       "\n",
       "   charge_under_50  charge_under_100  charge_under_500  charge_under_1000  \\\n",
       "0                1                 0                 0                  0   \n",
       "1                1                 0                 0                  0   \n",
       "2                0                 0                 0                  0   \n",
       "3                1                 0                 0                  0   \n",
       "4                1                 0                 0                  0   \n",
       "\n",
       "   charge_over_1000  is_credit     category  \n",
       "0                 0          0    groceries  \n",
       "1                 0          1  REDACTED_CC_ACCT credit  \n",
       "2                 0          0    household  \n",
       "3                 0          0   dining out  \n",
       "4                 0          0    household  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Capture predictions in CSV format\n",
    "csv_df = X.copy().reset_index(drop=True)\n",
    "csv_df[\"category\"] = y_predicted\n",
    "print(csv_df.shape)\n",
    "csv_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fiscal-mobile",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(147, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Account</th>\n",
       "      <th>Description</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Credit</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>REDACTED_CC_ACCT</td>\n",
       "      <td>AplPay MEIJER REDACTED_STATE</td>\n",
       "      <td>42.97</td>\n",
       "      <td>0</td>\n",
       "      <td>groceries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>REDACTED_CC_ACCT</td>\n",
       "      <td>REDACTED_CC_OFFER</td>\n",
       "      <td>-43.12</td>\n",
       "      <td>1</td>\n",
       "      <td>REDACTED_CC_ACCT credit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>REDACTED_CC_ACCT</td>\n",
       "      <td>AplPay TARGET REDACTED_STATE</td>\n",
       "      <td>9.08</td>\n",
       "      <td>0</td>\n",
       "      <td>household</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>REDACTED_CC_ACCT</td>\n",
       "      <td>NOODLES &amp; CO  REDACTED_STATE</td>\n",
       "      <td>31.42</td>\n",
       "      <td>0</td>\n",
       "      <td>dining out</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>REDACTED_CC_ACCT</td>\n",
       "      <td>AplPay IKEA REDACTED_STATE</td>\n",
       "      <td>42.58</td>\n",
       "      <td>0</td>\n",
       "      <td>household</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Account                                 Description Amount  Credit  \\\n",
       "0    REDACTED_CC_ACCT  AplPay MEIJER REDACTED_STATE  42.97       0   \n",
       "1    REDACTED_CC_ACCT              REDACTED_CC_OFFER -43.12       1   \n",
       "2    REDACTED_CC_ACCT  AplPay TARGET REDACTED_STATE   9.08       0   \n",
       "3    REDACTED_CC_ACCT  NOODLES & CO  REDACTED_STATE  31.42       0   \n",
       "4    REDACTED_CC_ACCT  AplPay IKEA REDACTED_STATE 42.58       0   \n",
       "\n",
       "      Category  \n",
       "0    groceries  \n",
       "1  REDACTED_CC_ACCT credit  \n",
       "2    household  \n",
       "3   dining out  \n",
       "4    household  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the human-readable CSV to disk with full charge data\n",
    "raw_df[\"Credit\"] = X[\"is_credit\"].copy()\n",
    "raw_df[\"Category\"] = csv_df[\"category\"].copy()\n",
    "print(raw_df.shape)\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ahead-sharing",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.to_csv(\"MAY21.csv\", index=False, sep=CSV_DELIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "motivated-brain",
   "metadata": {},
   "source": [
    "Note that when writing to the master data set, we are *appending* the data (unless there is no existing master set yet) so that we can collect this data over time and then retrain the model later on the full dataset - perhaps after going through and manually relabeling some entries to improve the model's learning - using **Pipeline 2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "qualified-deficit",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_data_to_master_set(csv_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assured-paradise",
   "metadata": {},
   "source": [
    "And just like that, no more hours spent classifying transactions!\n",
    "\n",
    "Thanks for reading through, and I hope this was a helpful toy project to you."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "representative-attendance",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acceptable-village",
   "metadata": {},
   "source": [
    "## Model Testing Sandbox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "editorial-virtue",
   "metadata": {},
   "source": [
    "I used this cell to do testing of different classifier variations using the One vs. Rest (OVR) strategy.\n",
    "\n",
    "**Warning**: Grid searching can take quite a long time for these different classifiers and parameters, so if you run this cell, be prepared to step away for several minutes to an hour.\n",
    "\n",
    "Please share any advice on the approaches discussed above and in the cell below - I'm a noob and there are probably better classifiers to test or ways to evaluate them than this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "effective-circumstances",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rfc F1 Score is 0.3473845761208306\n",
      "\n",
      "\n",
      "lrc F1 Score is 0.3609711455509903\n",
      "\n",
      "\n",
      "svc F1 Score is 0.3751328276510206\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# One-hot encode y target values for use with OvR and resplit data\n",
    "y_test = get_dummies(\n",
    "    raw_df[\"category\"].str.lower(), prefix=\"\", prefix_sep=\"\", columns=[\"category\"]\n",
    ")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_data, y_data, train_size=0.8, test_size=0.2\n",
    ")\n",
    "\n",
    "\n",
    "# Build TF-IDF pipeline for description texts, passing other columns \"as-is\"\n",
    "tfidf_vec = TfidfVectorizer()\n",
    "ct = ColumnTransformer([(\"tfidf\", tfidf_vec, \"description\")], remainder=\"passthrough\")\n",
    "\n",
    "clfs = [\n",
    "    {\n",
    "        \"name\": \"rfc\",\n",
    "        \"clf\": OneVsRestClassifier(RandomForestClassifier()),\n",
    "        \"params\": {\n",
    "            \"coltrans__tfidf__ngram_range\": [(1, 1), (1, 2), (1, 3), (1, 4)],\n",
    "            \"coltrans__tfidf__norm\": [\"l1\", \"l2\"],\n",
    "            \"clf__estimator__max_features\": [\"auto\", \"sqrt\"],\n",
    "            \"clf__estimator__n_estimators\": [200, 1000],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"lrc\",\n",
    "        \"clf\": OneVsRestClassifier(LogisticRegression()),\n",
    "        \"params\": {\n",
    "            \"coltrans__tfidf__ngram_range\": [(1, 1), (1, 2), (1, 3), (1, 4)],\n",
    "            \"coltrans__tfidf__norm\": [\"l1\", \"l2\"],\n",
    "            \"clf__estimator__C\": logspace(-2, 2, 5),\n",
    "            \"clf__estimator__penalty\": [\"l1\", \"l2\"],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"svc\",\n",
    "        \"clf\": OneVsRestClassifier(SVC()),\n",
    "        \"params\": {\n",
    "            \"coltrans__tfidf__ngram_range\": [(1, 1), (1, 2), (1, 3), (1, 4)],\n",
    "            \"coltrans__tfidf__norm\": [\"l1\", \"l2\"],\n",
    "            \"clf__estimator__C\": logspace(-2, 2, 5),\n",
    "            \"clf__estimator__gamma\": logspace(-2, 2, 5),\n",
    "            \"clf__estimator__kernel\": [\"rbf\"],\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "for clf in clfs:\n",
    "    pipe = Pipeline([(\"coltrans\", ct), (\"clf\", clf[\"clf\"])])\n",
    "    best_clf = GridSearchCV(\n",
    "        pipe, param_grid=clf[\"params\"], scoring=\"f1_macro\", n_jobs=-1\n",
    "    ).fit(X_train, y_train)\n",
    "    print(\"{} F1 Score is {}\\n\\n\".format(clf[\"name\"], best_clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naughty-lawsuit",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
